---
title: "Simulations"
output: github_document
date: "2022-11-03"
---

```{r setup, include = FALSE}
library(tidyverse)
set.seed(1)

knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Simulations!!

Here's our function from before.

```{r}
sim_mean_sd = function(n_obs, mu = 7, sigma = 4) {
  
  x = rnorm(n = n_obs, mean = mu, sd = sigma)

tibble(
  mu_hat = mean(x), 
  sigma_hat = sd(x)
)
}
```

How did we use this before?

```{r}
sim_mean_sd(n_obs = 30)
```
Every time we run this function, we get different estimates. Can we do this 100 times? 1000 times?

How can we use this now...a for loop perhaps?
We can create a vector with 100 empty spots, and fill these columns with `sim_mean_sd`.

```{r}
output = vector("list", length = 100)

for (i in 1:100) {
  output[[i]] = sim_mean_sd(n_obs = 30)
  
}

bind_rows(output)
```


Let's use list columns instead! `expand_grid` gives us all possible combinations of sample size = 30 and iteration from 1 to 100. Then, we use `map` to input the results of `sim_mean_sd` into our df.

```{r}
sim_results_df = 
  expand_grid(
    sample_size = 30,
    iteration = 1:100
  ) %>% 
  mutate(
    estimate_df = map(sample_size, sim_mean_sd)
  ) %>% 
  unnest(estimate_df)

sim_results_df
```

Let's plot some summaries for our simulation results.

```{r}
sim_results_df %>% 
  ggplot(aes(x = mu_hat)) + 
  geom_density()
```


What if we changed the sample sizes, and simulated 1000 times?

```{r}
sim_results_df = expand_grid(
    sample_size = c(30, 60, 120, 240), 
    iteration = 1:1000
  ) %>% 
    mutate(
      estimate_df = map(sample_size, sim_mean_sd)
    ) %>% 
    unnest(estimate_df)
```

```{r}
sim_results_df %>% 
  mutate(sample_size = str_c("N = ", sample_size), 
         sample_size = fct_inorder(sample_size)) %>% 
  ggplot(aes(x = sample_size, y = mu_hat)) +
  geom_violin()
```

As we expect, our distribution is much wider with a larger sample size and more spread out with a narrower sample size

```{r}
sim_results_df %>% 
  mutate(sample_size = str_c("N = ", sample_size), 
         sample_size = fct_inorder(sample_size)) %>% 
  group_by(sample_size) %>% 
  summarise(emp_st_err = sd(mu_hat))

4 / sqrt(120) # This is close to what we got in our df
```

## Let's see two inputs...

Let's try varying two simulation parameters

```{r, cache = TRUE}
sim_results_df = expand_grid(
  sample_size = c(30, 60, 120, 240), 
  true_sigma = c(6, 3),
  iteration = 1:1000
  ) %>% 
  mutate(
     estimate_df = map2(.x = sample_size, .y = true_sigma, ~sim_mean_sd(n_obs = .x, sigma = .y))
  ) %>% 
  unnest(estimate_df)

```

```{r}
sim_results_df %>% 
  mutate(sample_size = str_c("N = ", sample_size), 
         sample_size = fct_inorder(sample_size)) %>% 
  ggplot(aes(x = sample_size, y = mu_hat)) + 
  geom_violin() + 
  facet_grid(. ~ true_sigma)
```

Note knitting simulations may take time. `cache = TRUE` saves results when you first run it and doesn't re-run it each time you knit. It doesn't run again unless you change the code chunk. Note this may be an issue when your code relies on previous changes. 

